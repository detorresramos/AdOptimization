{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13748836-8496-421a-a2a1-58855a476198",
   "metadata": {},
   "source": [
    "# Ad Optimization Hackathon - Aimpoint Digital"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5772359-a855-43ba-b9e9-2c0dbb471ad2",
   "metadata": {},
   "source": [
    "## Hackathon Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93591af5-99d4-4281-9a80-5a6079ab9bc5",
   "metadata": {},
   "source": [
    "For this Hackathon, you will be querying an API that contains data on 5 different ads (ad_1, ad_2, ad_3, ad_4 and ad_5). Each query of the API for a certain ad outputs a '1' or '0'. '1' means the ad was clicked on and '0' means the ad was not clicked on. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8378a4c",
   "metadata": {},
   "source": [
    "The following code queries the API to output 10 datapoints for all 5 ads in Python. You may also use the following code as starter code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77ceabb2-1e30-4b9f-87b9-5a54970a0243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ad_outcomes for  ad_1  =  ['1', '0', '0', '1', '0', '0', '1', '0', '1', '1']\n",
      "ad_outcomes for  ad_2  =  ['1', '1', '0', '1', '1', '0', '0', '1', '0', '1']\n",
      "ad_outcomes for  ad_3  =  ['1', '0', '0', '0', '1', '1', '1', '1', '1', '1']\n",
      "ad_outcomes for  ad_4  =  ['0', '0', '0', '0', '0', '1', '0', '0', '0', '0']\n",
      "ad_outcomes for  ad_5  =  ['0', '1', '0', '1', '0', '0', '0', '1', '0', '1']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "N = 10 # Set how many data points you would like. Querying the API for a single data point takes ~ 0.2 s. \n",
    "ad_list = ['ad_1', 'ad_2', 'ad_3', 'ad_4', 'ad_5'] \n",
    "for ad in ad_list:\n",
    "    ad_outcome = []\n",
    "    for i in range (N):\n",
    "        ad_gen = \"https://755vgi76zg.execute-api.us-east-1.amazonaws.com/prod\"\n",
    "        data = {\n",
    "            \"ad\": ad\n",
    "        }\n",
    "\n",
    "        response = requests.post(ad_gen, json.dumps(data))\n",
    "        resp = json.loads(response.content)\n",
    "        generate_ad = resp[\"body\"]\n",
    "        ad_outcome.append(generate_ad[1])\n",
    "    print ('ad_outcomes for ', ad, ' = ', ad_outcome)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1012a11",
   "metadata": {},
   "source": [
    "The following are some generic API instructions if you choose to use a language other than Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94111397",
   "metadata": {},
   "source": [
    "**URL** : https://755vgi76zg.execute-api.us-east-1.amazonaws.com/prod\n",
    "\n",
    "**HTTP Method** : POST\n",
    "\n",
    "**Request Body** : {'ad': 'ad_1'} \n",
    "\n",
    "The key ad is the particular ad you want the outcome for. The values can be 'ad_1', 'ad_2', ..., 'ad_5' depending on which ad you're querying.\n",
    "\n",
    "**Format** : \n",
    "The POST method will require the user to send the URL and the Request Body to the API. \n",
    "\n",
    "**Response**: '{\"statusCode\": 200, \"body\": \"[1]\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9105fac-2d4e-46d4-9690-411ec31a24e3",
   "metadata": {},
   "source": [
    "### Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdf4193-443e-4a1b-ba6d-1efab9ad30e6",
   "metadata": {},
   "source": [
    "Here are some comments on the distributions and the rewards:\n",
    "\n",
    "1. The distributions for all 5 ads remain constant and do not change over time.\n",
    "2. Every time an ad is clicked on, you end up with a reward of +1. We do not have data on whether the ad ends up in conversion.\n",
    "3. Contestants can make as many API calls as they like. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f4760-722a-467d-a1cd-2fccc7ea8d64",
   "metadata": {},
   "source": [
    "### Objective\n",
    "\n",
    "\n",
    "1. You will submit your well-documented code in an easily executable format.\n",
    "2. You will also submit a short video (screen-recorded) of your approach and results by the end of the hackathon.\n",
    "3. Evalution will be done on **'reward obtained per 1000 steps'**. Please report this in the demo and in your code submission.\n",
    "\n",
    "Good Luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5fc34c-de8f-496d-99f2-02ed88e1515a",
   "metadata": {},
   "source": [
    "## Supplementary Reading (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b5b564-73da-4450-90f0-c186efe0348a",
   "metadata": {},
   "source": [
    "### This is some additional information that you might find useful. You are not required to use this for the hackathon and this is optional reading!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a912dfae-7284-40b9-89dc-e0d8273d6f2a",
   "metadata": {},
   "source": [
    "For this hackathon, we will be exploring a technique called Multi-Armed Bandits (MABs). \n",
    "\n",
    "To understand MABs, let's consider an example. We are all going on a vacation to Las Vegas, and we plan on trying our luck at the casinos. We want to play the slot machines, and as with any form of gambling, we want to maximize our rewards at the end of the day. The casino we go to has $K$ slot machines corresponding to K levers that we can pull. The reward for each machine is drawn from the following reward distributions corresponding to each lever $$B = \\{R_1, R_2, R_3, ... ,R_K\\} \\,\\,\\,\\,\\,\\, where\\, K\\, \\in\\,\\, \\mathbf{N}^+$$\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86bbe39-2bcb-403f-9b11-1ea8b7d5cb4d",
   "metadata": {},
   "source": [
    "Let $\\mu_1, ..., \\mu_K$ be the true means associated with each of the reward distributions. Obviously, we don't know these values for gambling would then be child's play. We do, however, want to correctly approximate the means of these reward distributions so that we may exploit those levers that correspond to the maximum reward. Ideally, we'd want to do this in as few steps as possible. Formally, the number of steps we have remaining is called *Horizon* ($H$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0dea3f-0fdb-4b44-a756-c2923d1d8a8e",
   "metadata": {},
   "source": [
    "How do we quantify how good our strategy is? The quality of our strategy is measured by a metric called *Regret* $(\\rho)$ as defined by the following formula $$ \\rho = T\\mu^* - \\sum_{t=1}^{T} \\hat{r_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8be9d66-bda6-4a53-bcde-3d5e858ff362",
   "metadata": {},
   "source": [
    "where $\\mu^* = \\displaystyle\\max_{k} \\{\\mu_k\\}$ or the maximal reward mean across levers and $T$ is the number of pulls on the lever. This is synonymous with the lever offering the greatest reward. We don't know which lever does this presently, but we want to correctly converge towards it. $\\hat{r_t}$ is the reward in round $t$. This refers to the reward from a potentially sub-optimal lever we are operating in the present. In plain English, $\\rho$ tells us how close/far we are from identifying the optimal lever. If $\\rho$ is large, then we are not close to identifying the optimal lever. If $\\rho$ is consistently large over time, it means that our algorithm has converged on a sub-optimal solution, and we are stuck there. Ideally, we want $\\rho$ to decrease and tend towards 0 over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1843ec77-2d23-413a-b6c2-c454f9de82a0",
   "metadata": {},
   "source": [
    "I'm sure all of you must've realized by now that bandits are an exploration Vs exploitation problem. Explore forever, and you might someday find the optimal lever - but your losses will be too large to recuperate from. Exploit from the start, and you'll easily get stuck at a sub-optimal solution. Winning this hackathon will require dealing with the exploration Vs exploitation dilemma tactfully and this will allow you to minimize cumulative $\\rho$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd45c2-ac91-4562-ace3-fada68f84958",
   "metadata": {},
   "source": [
    "### Multi-Armed Bandits : A Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e3dc6-cf84-4797-a8ca-340d58d42abb",
   "metadata": {},
   "source": [
    "Let's understand how to approach a MAB with an example using sample data. The example and the data for this example has been sourced from [here](https://www.analyticsvidhya.com/blog/2018/09/reinforcement-multi-armed-bandit-scratch-python/). We have 10 different ads that can be run on our website and we some historic information on which ads were clicked by users. '1' represents a click and '0' represents no click. We want to identify which ads have the highest likelihood to be clicked by a new user to our website. This is very similar to the slot machine problem, except we are trying to optimize ads to maximize Click Through Rate (CTR). You can find a sample of the dataset below. The complete dataset can be found [here](https://drive.google.com/file/d/1whkIInL4FKeHg2IfdcbT1j18L26fg9aF/view). Make sure to download the whole dataset in order to implement the code snippets in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa4401-5fdf-4d8d-97ba-f7b1d9f0a946",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"460\" height=\"460\" src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2018/09/im_24.jpg\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68e61f0-a148-4c31-8e65-b7f34f155f32",
   "metadata": {},
   "source": [
    "### Random Sampling - Simple Approach to MABs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9209f0cf-425e-423a-9740-1e379a3af520",
   "metadata": {},
   "source": [
    "We will start with the simplest solution to the exploration vs exploitation dilemma - random sampling. It can be argued that this isn't a solution at all - but its good place to start from and set a solid baseline to compare other techniques with. Every time a user comes to our website, we randomly show them one of the ads. We do this experiment 10,000 times and record the total reward from this experiment. The following code snippet implements this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ace486e-7a92-4aff-b7bc-4b27c94879c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward from Random Sampling:  1248\n"
     ]
    }
   ],
   "source": [
    "# Random Selection\n",
    "\n",
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Ads_Optimisation.csv')\n",
    "\n",
    "# Implementing Random Selection\n",
    "import random\n",
    "random.seed(13)\n",
    "N = 10000\n",
    "d = 10\n",
    "ads_selected = []\n",
    "total_reward = 0\n",
    "for n in range(0, N):\n",
    "    ad = random.randrange(d)\n",
    "    ads_selected.append(ad)\n",
    "    reward = dataset.values[n, ad]\n",
    "    total_reward = total_reward + reward\n",
    "    \n",
    "print ('Total Reward from Random Sampling: ', total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fc2f6f-7354-493d-955b-b8187c15734b",
   "metadata": {},
   "source": [
    "We have received a total reward of 1248. Is this any good? We don't know for sure yet. But its very likely a poor, sub-optimal result. We can tackle the exploration vs exploitation dilemma in a manner that is much better than simply picking random actions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
